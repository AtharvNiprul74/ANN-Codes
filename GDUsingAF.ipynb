{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtYI1NgQ6kxWQoaRRcfM19",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtharvNiprul74/ANN-Codes/blob/main/GDUsingAF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Student Score Prediction.**\n"
      ],
      "metadata": {
        "id": "1sr0UPXfbZC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay4y1NhuBTBr",
        "outputId": "e23a126e-072a-4e10-a11f-13e04de2e35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.          0.1010101   0.2020202   0.3030303   0.4040404   0.50505051\n",
            "  0.60606061  0.70707071  0.80808081  0.90909091  1.01010101  1.11111111\n",
            "  1.21212121  1.31313131  1.41414141  1.51515152  1.61616162  1.71717172\n",
            "  1.81818182  1.91919192  2.02020202  2.12121212  2.22222222  2.32323232\n",
            "  2.42424242  2.52525253  2.62626263  2.72727273  2.82828283  2.92929293\n",
            "  3.03030303  3.13131313  3.23232323  3.33333333  3.43434343  3.53535354\n",
            "  3.63636364  3.73737374  3.83838384  3.93939394  4.04040404  4.14141414\n",
            "  4.24242424  4.34343434  4.44444444  4.54545455  4.64646465  4.74747475\n",
            "  4.84848485  4.94949495  5.05050505  5.15151515  5.25252525  5.35353535\n",
            "  5.45454545  5.55555556  5.65656566  5.75757576  5.85858586  5.95959596\n",
            "  6.06060606  6.16161616  6.26262626  6.36363636  6.46464646  6.56565657\n",
            "  6.66666667  6.76767677  6.86868687  6.96969697  7.07070707  7.17171717\n",
            "  7.27272727  7.37373737  7.47474747  7.57575758  7.67676768  7.77777778\n",
            "  7.87878788  7.97979798  8.08080808  8.18181818  8.28282828  8.38383838\n",
            "  8.48484848  8.58585859  8.68686869  8.78787879  8.88888889  8.98989899\n",
            "  9.09090909  9.19191919  9.29292929  9.39393939  9.49494949  9.5959596\n",
            "  9.6969697   9.7979798   9.8989899  10.        ]\n",
            "[ 0.99342831  0.2285219   2.30547809  4.56121123  1.55189527  2.05697861\n",
            "  6.18872866  5.07022299  3.10145527  5.63057463  4.12366966  4.62409605\n",
            "  6.5445306   2.73909608  3.62087141  6.45118252  6.05514584  9.21435325\n",
            "  7.27486094  6.77135219 13.03230764 10.15450801 11.24616752  8.76666524\n",
            " 11.03244667 12.84810781 10.82932598 14.38775967 12.94013676 14.06307715\n",
            " 13.94810193 19.36112203 16.13462171 14.55124481 18.816807   15.23508038\n",
            " 18.59954537 14.76752844 16.53554709 20.09069217 21.67895336 21.04980727\n",
            " 20.98082465 21.11496433 19.26517824 21.28758431 22.31104569 25.85161819\n",
            " 24.92966082 21.22139444 25.90069319 24.9874112  24.90878226 27.99102935\n",
            " 29.33472632 29.64033802 26.60439324 28.16945404 29.95545616 31.74907005\n",
            " 29.34468183 30.43676285 29.10046137 29.42576857 33.94828397 35.54076289\n",
            " 33.18931309 35.84544963 35.06670639 33.55824534 36.07632656 38.93465899\n",
            " 36.29198429 39.99797418 32.13424717 39.52259289 38.55793252 38.29087419\n",
            " 39.57746095 35.92385207 39.96469663 41.62331605 44.3699295  40.88265148\n",
            " 40.80725522 41.92577884 45.26514767 44.59689616 43.38492404 45.97602982\n",
            " 45.64870055 47.89688594 45.06054028 46.31437268 46.69053117 45.05276808\n",
            " 49.07708904 49.51200953 49.50517641 49.53082573]\n",
            "[-1.71481604 -1.68017329 -1.64553055 -1.6108878  -1.57624505 -1.5416023\n",
            " -1.50695955 -1.4723168  -1.43767406 -1.40303131 -1.36838856 -1.33374581\n",
            " -1.29910306 -1.26446031 -1.22981757 -1.19517482 -1.16053207 -1.12588932\n",
            " -1.09124657 -1.05660382 -1.02196108 -0.98731833 -0.95267558 -0.91803283\n",
            " -0.88339008 -0.84874733 -0.81410459 -0.77946184 -0.74481909 -0.71017634\n",
            " -0.67553359 -0.64089084 -0.6062481  -0.57160535 -0.5369626  -0.50231985\n",
            " -0.4676771  -0.43303435 -0.39839161 -0.36374886 -0.32910611 -0.29446336\n",
            " -0.25982061 -0.22517786 -0.19053512 -0.15589237 -0.12124962 -0.08660687\n",
            " -0.05196412 -0.01732137  0.01732137  0.05196412  0.08660687  0.12124962\n",
            "  0.15589237  0.19053512  0.22517786  0.25982061  0.29446336  0.32910611\n",
            "  0.36374886  0.39839161  0.43303435  0.4676771   0.50231985  0.5369626\n",
            "  0.57160535  0.6062481   0.64089084  0.67553359  0.71017634  0.74481909\n",
            "  0.77946184  0.81410459  0.84874733  0.88339008  0.91803283  0.95267558\n",
            "  0.98731833  1.02196108  1.05660382  1.09124657  1.12588932  1.16053207\n",
            "  1.19517482  1.22981757  1.26446031  1.29910306  1.33374581  1.36838856\n",
            "  1.40303131  1.43767406  1.4723168   1.50695955  1.5416023   1.57624505\n",
            "  1.6108878   1.64553055  1.68017329  1.71481604]\n",
            "[-1.61129629 -1.66308406 -1.52246433 -1.36974056 -1.57348544 -1.53928892\n",
            " -1.25954996 -1.33527807 -1.46857292 -1.29733962 -1.39936419 -1.36548296\n",
            " -1.23546049 -1.49310635 -1.433406   -1.24178059 -1.26859415 -1.05470091\n",
            " -1.18601368 -1.2201036  -0.7962074  -0.991048   -0.9171375  -1.0850115\n",
            " -0.93160741 -0.80867859 -0.94535964 -0.7044369  -0.8024478  -0.72641944\n",
            " -0.73420381 -0.36771683 -0.58616611 -0.69336819 -0.40456952 -0.6470693\n",
            " -0.41927916 -0.67872477 -0.55902157 -0.31832149 -0.21078872 -0.25338487\n",
            " -0.25805532 -0.24897343 -0.37421267 -0.23728625 -0.16799309  0.07172036\n",
            "  0.0092995  -0.24176761  0.07504297  0.01320948  0.00788593  0.21656859\n",
            "  0.30754321  0.32823456  0.12268678  0.22864878  0.34956954  0.47100566\n",
            "  0.30821724  0.38215627  0.29168236  0.3137072   0.61990281  0.72772114\n",
            "  0.5685169   0.74834987  0.69562528  0.59349536  0.76398133  0.95750391\n",
            "  0.77858237  1.02949536  0.49708397  0.9973098   0.93199775  0.91391664\n",
            "  1.00102463  0.75365808  1.02724231  1.13953867  1.32549735  1.08939218\n",
            "  1.0842875   1.16001682  1.38610784  1.34086406  1.25880783  1.43423791\n",
            "  1.41207617  1.56428892  1.37225495  1.45714532  1.48261302  1.37172874\n",
            "  1.64419424  1.67364041  1.67317777  1.67491435]\n",
            "Training for sigmoid:\n",
            "Weight=0.6937635649750347,Bias=-1.0777733168343315\n",
            "Normalized value= 1.610887798481921\n",
            "Output for sigmoid= 0.5099491728361508\n",
            "\n",
            "\n",
            "Training for relu:\n",
            "Weight=1.4452473996414923,Bias=-0.9420947757514703\n",
            "Normalized value= 1.610887798481921\n",
            "1.3860366261187345\n",
            "Output for Relu= 1.3860366261187345\n",
            "\n",
            "\n",
            "Training for tanh:\n",
            "Weight=0.9952958773555384,Bias=0.0883430444189243\n",
            "Normalized value= 1.610887798481921\n",
            "Output= 0.9343574302461456\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# generating data set randomly\n",
        "study_hours=np.linspace(0,10,100)\n",
        "print(study_hours)\n",
        "score=5*study_hours + np.random.normal(0,2,100)\n",
        "print(score)\n",
        "\n",
        "# normalizing data set\n",
        "norm_study_hours=(study_hours-np.mean(study_hours))/np.std(study_hours)\n",
        "print(norm_study_hours)\n",
        "norm_score=(score-np.mean(score))/np.std(score)\n",
        "print(norm_score)\n",
        "\n",
        "# gradient descent\n",
        "learning_rate=0.001\n",
        "\n",
        "def graident(act_fun,itrs=1000):\n",
        "  w=np.random.randn()\n",
        "  b=np.random.randn()\n",
        "  for i in range(itrs):\n",
        "    norm_score_pred=act_fun((w*norm_study_hours)+b)\n",
        "    error=(norm_score-norm_score_pred)\n",
        "\n",
        "    dw=-(2/len(norm_study_hours))*np.sum(norm_study_hours*error)\n",
        "    db=-(2/len(norm_study_hours))*np.sum(error)\n",
        "\n",
        "    w=w-learning_rate*dw\n",
        "    b=b-learning_rate*db\n",
        "\n",
        "  return w,b\n",
        "\n",
        "# sigmoid.\n",
        "w_sigmoid,b_sigmoid=graident(sigmoid)\n",
        "print(\"Training for sigmoid:\")\n",
        "print(f\"Weight={w_sigmoid},Bias={b_sigmoid}\")\n",
        "\n",
        "new_hour= 9.6969697\n",
        "normalized=(new_hour-np.mean(study_hours))/np.std(study_hours)\n",
        "print(\"Normalized value=\",normalized)\n",
        "print(\"Output for sigmoid=\",sigmoid((w_sigmoid*normalized)+b_sigmoid))\n",
        "print(\"\\n\")\n",
        "\n",
        "# relu.\n",
        "w_relu,b_relu=graident(relu)\n",
        "print(\"Training for relu:\")\n",
        "print(f\"Weight={w_relu},Bias={b_relu}\")\n",
        "\n",
        "new_hour= 9.6969697\n",
        "normalized=(new_hour-np.mean(study_hours))/np.std(study_hours)\n",
        "print(\"Normalized value=\",normalized)\n",
        "print((w_relu*normalized)+b_relu)\n",
        "print(\"Output for Relu=\",relu((w_relu*normalized)+b_relu))\n",
        "print(\"\\n\")\n",
        "\n",
        "# tanh\n",
        "w_tanh,b_tanh=graident(tanh)\n",
        "print(\"Training for tanh:\")\n",
        "print(f\"Weight={w_tanh},Bias={b_tanh}\")\n",
        "\n",
        "new_hour= 9.6969697\n",
        "normalized=(new_hour-np.mean(study_hours))/np.std(study_hours)\n",
        "print(\"Normalized value=\",normalized)\n",
        "print(\"Output=\",tanh((w_tanh*normalized)+b_tanh))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# generating data set randomly\n",
        "study_hours=np.linspace(0,10,100)\n",
        "# print(study_hours)\n",
        "score=5*study_hours + np.random.normal(0,2,100)\n",
        "# print(score)\n",
        "\n",
        "# normalizing data set\n",
        "norm_study_hours=(study_hours-np.mean(study_hours))/np.std(study_hours)\n",
        "# print(norm_study_hours)\n",
        "norm_score=(score-np.mean(score))/np.std(score)\n",
        "print(norm_score)\n",
        "\n",
        "# gradient descent\n",
        "w=np.random.randn()\n",
        "b=np.random.randn()\n",
        "learning_rate=0.001\n",
        "\n",
        "def graident(act_fun,itrs=1750):\n",
        "  global w,b\n",
        "  for i in range(itrs):\n",
        "    norm_score_pred=act_fun((w*norm_study_hours)+b)\n",
        "    error=(norm_score-norm_score_pred)\n",
        "\n",
        "\n",
        "    dw=-(2/len(norm_study_hours))*np.sum(norm_study_hours*error)\n",
        "    db=-(2/len(norm_study_hours))*np.sum(error)\n",
        "\n",
        "    w=w-learning_rate*dw\n",
        "    b=b-learning_rate*db\n",
        "\n",
        "    # if(i%100==0):\n",
        "    #     print(np.mean(error))\n",
        "\n",
        "  return w,b\n",
        "\n",
        "# sigmoid.\n",
        "w_sigmoid,b_sigmoid=graident(sigmoid)\n",
        "print(\"Training for sigmoid:\")\n",
        "print(f\"Weight={w_sigmoid},Bias={b_sigmoid}\")\n",
        "\n",
        "new_hour= 9.6969697\n",
        "normalized=(new_hour-np.mean(study_hours))/np.std(study_hours)\n",
        "print(\"Normalized value=\",normalized)\n",
        "print(\"Output for sigmoid=\",sigmoid((w_sigmoid*normalized)+b_sigmoid))\n",
        "print(\"\\n\")\n",
        "\n",
        "# relu.\n",
        "w_relu,b_relu=graident(relu)\n",
        "print(\"Training for relu:\")\n",
        "print(f\"Weight={w_relu},Bias={b_relu}\")\n",
        "\n",
        "new_hour= 0.1010101\n",
        "normalized=(new_hour-np.mean(study_hours))/np.std(study_hours)\n",
        "print(\"Normalized value=\",normalized)\n",
        "print((w_relu*normalized)+b_relu)\n",
        "print(\"Output for Relu=\",relu((w_relu*normalized)+b_relu))\n",
        "print(\"\\n\")\n",
        "\n",
        "# tanh\n",
        "w_tanh,b_tanh=graident(tanh)\n",
        "print(\"Training for tanh:\")\n",
        "print(f\"Weight={w_tanh},Bias={b_tanh}\")\n",
        "\n",
        "new_hour= 9.6969697\n",
        "normalized=(new_hour-np.mean(study_hours))/np.std(study_hours)\n",
        "print(\"Normalized value=\",normalized)\n",
        "print(\"Output=\",tanh((w_tanh*normalized)+b_tanh))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kSWZexzBw4_",
        "outputId": "b45b4826-a8e4-4fa4-ea7f-9de18fb17629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.61129629 -1.66308406 -1.52246433 -1.36974056 -1.57348544 -1.53928892\n",
            " -1.25954996 -1.33527807 -1.46857292 -1.29733962 -1.39936419 -1.36548296\n",
            " -1.23546049 -1.49310635 -1.433406   -1.24178059 -1.26859415 -1.05470091\n",
            " -1.18601368 -1.2201036  -0.7962074  -0.991048   -0.9171375  -1.0850115\n",
            " -0.93160741 -0.80867859 -0.94535964 -0.7044369  -0.8024478  -0.72641944\n",
            " -0.73420381 -0.36771683 -0.58616611 -0.69336819 -0.40456952 -0.6470693\n",
            " -0.41927916 -0.67872477 -0.55902157 -0.31832149 -0.21078872 -0.25338487\n",
            " -0.25805532 -0.24897343 -0.37421267 -0.23728625 -0.16799309  0.07172036\n",
            "  0.0092995  -0.24176761  0.07504297  0.01320948  0.00788593  0.21656859\n",
            "  0.30754321  0.32823456  0.12268678  0.22864878  0.34956954  0.47100566\n",
            "  0.30821724  0.38215627  0.29168236  0.3137072   0.61990281  0.72772114\n",
            "  0.5685169   0.74834987  0.69562528  0.59349536  0.76398133  0.95750391\n",
            "  0.77858237  1.02949536  0.49708397  0.9973098   0.93199775  0.91391664\n",
            "  1.00102463  0.75365808  1.02724231  1.13953867  1.32549735  1.08939218\n",
            "  1.0842875   1.16001682  1.38610784  1.34086406  1.25880783  1.43423791\n",
            "  1.41207617  1.56428892  1.37225495  1.45714532  1.48261302  1.37172874\n",
            "  1.64419424  1.67364041  1.67317777  1.67491435]\n",
            "Training for sigmoid:\n",
            "Weight=1.8724884512049986,Bias=-1.499784150555288\n",
            "Normalized value= 1.610887798481921\n",
            "Output for sigmoid= 0.8200350020002536\n",
            "\n",
            "\n",
            "Training for relu:\n",
            "Weight=3.4974189889442733,Bias=-2.812578394993389\n",
            "Normalized value= -1.6801732944532655\n",
            "-8.688848379731297\n",
            "Output for Relu= 0.0\n",
            "\n",
            "\n",
            "Training for tanh:\n",
            "Weight=4.323194933954555,Bias=-1.6832905259459852\n",
            "Normalized value= 1.610887798481921\n",
            "Output= 0.9999482280257627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "study_hours = np.linspace(0, 10, 100)\n",
        "print(study_hours)\n",
        "score = 5 * study_hours + np.random.normal(0, 2, 100)\n",
        "print(score)\n",
        "\n",
        "new_study_hours = (study_hours - np.mean(study_hours)) / np.std(study_hours)\n",
        "print(new_study_hours)\n",
        "new_score = (score - np.mean(score)) / np.std(score)\n",
        "print(new_score)\n",
        "\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "learning_rate = 0.001\n",
        "\n",
        "def gradient_descent(act_fun, itrs=1750):\n",
        "    global w, b\n",
        "    for i in range(itrs):\n",
        "        norm_score_pred = act_fun((w * new_study_hours) + b)\n",
        "        error = (new_score - norm_score_pred)\n",
        "\n",
        "        dw = -(2 / len(new_study_hours)) * np.sum(new_study_hours * error)\n",
        "        db = -(2 / len(new_study_hours)) * np.sum(error)\n",
        "\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "    return w, b\n",
        "\n",
        "w_relu, b_relu = gradient_descent(relu)\n",
        "print(\"Training for relu:\")\n",
        "print(f\"Weight={w_relu}, Bias={b_relu}\")\n",
        "\n",
        "test_study_hours = 9.6969697\n",
        "\n",
        "normalized_test_study_hours = (test_study_hours - np.mean(study_hours)) / np.std(study_hours)\n",
        "print(\"Normalized Test Study Hours:\", normalized_test_study_hours)\n",
        "\n",
        "predicted_score_relu = relu(normalized_test_study_hours * w_relu + b_relu)\n",
        "print(f\"Predicted score (relu): {predicted_score_relu}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65g7wLl0I5Wq",
        "outputId": "9b078b1b-9bde-4703-d272-e21f752d70d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.          0.1010101   0.2020202   0.3030303   0.4040404   0.50505051\n",
            "  0.60606061  0.70707071  0.80808081  0.90909091  1.01010101  1.11111111\n",
            "  1.21212121  1.31313131  1.41414141  1.51515152  1.61616162  1.71717172\n",
            "  1.81818182  1.91919192  2.02020202  2.12121212  2.22222222  2.32323232\n",
            "  2.42424242  2.52525253  2.62626263  2.72727273  2.82828283  2.92929293\n",
            "  3.03030303  3.13131313  3.23232323  3.33333333  3.43434343  3.53535354\n",
            "  3.63636364  3.73737374  3.83838384  3.93939394  4.04040404  4.14141414\n",
            "  4.24242424  4.34343434  4.44444444  4.54545455  4.64646465  4.74747475\n",
            "  4.84848485  4.94949495  5.05050505  5.15151515  5.25252525  5.35353535\n",
            "  5.45454545  5.55555556  5.65656566  5.75757576  5.85858586  5.95959596\n",
            "  6.06060606  6.16161616  6.26262626  6.36363636  6.46464646  6.56565657\n",
            "  6.66666667  6.76767677  6.86868687  6.96969697  7.07070707  7.17171717\n",
            "  7.27272727  7.37373737  7.47474747  7.57575758  7.67676768  7.77777778\n",
            "  7.87878788  7.97979798  8.08080808  8.18181818  8.28282828  8.38383838\n",
            "  8.48484848  8.58585859  8.68686869  8.78787879  8.88888889  8.98989899\n",
            "  9.09090909  9.19191919  9.29292929  9.39393939  9.49494949  9.5959596\n",
            "  9.6969697   9.7979798   9.8989899  10.        ]\n",
            "[ 0.99342831  0.2285219   2.30547809  4.56121123  1.55189527  2.05697861\n",
            "  6.18872866  5.07022299  3.10145527  5.63057463  4.12366966  4.62409605\n",
            "  6.5445306   2.73909608  3.62087141  6.45118252  6.05514584  9.21435325\n",
            "  7.27486094  6.77135219 13.03230764 10.15450801 11.24616752  8.76666524\n",
            " 11.03244667 12.84810781 10.82932598 14.38775967 12.94013676 14.06307715\n",
            " 13.94810193 19.36112203 16.13462171 14.55124481 18.816807   15.23508038\n",
            " 18.59954537 14.76752844 16.53554709 20.09069217 21.67895336 21.04980727\n",
            " 20.98082465 21.11496433 19.26517824 21.28758431 22.31104569 25.85161819\n",
            " 24.92966082 21.22139444 25.90069319 24.9874112  24.90878226 27.99102935\n",
            " 29.33472632 29.64033802 26.60439324 28.16945404 29.95545616 31.74907005\n",
            " 29.34468183 30.43676285 29.10046137 29.42576857 33.94828397 35.54076289\n",
            " 33.18931309 35.84544963 35.06670639 33.55824534 36.07632656 38.93465899\n",
            " 36.29198429 39.99797418 32.13424717 39.52259289 38.55793252 38.29087419\n",
            " 39.57746095 35.92385207 39.96469663 41.62331605 44.3699295  40.88265148\n",
            " 40.80725522 41.92577884 45.26514767 44.59689616 43.38492404 45.97602982\n",
            " 45.64870055 47.89688594 45.06054028 46.31437268 46.69053117 45.05276808\n",
            " 49.07708904 49.51200953 49.50517641 49.53082573]\n",
            "[-1.71481604 -1.68017329 -1.64553055 -1.6108878  -1.57624505 -1.5416023\n",
            " -1.50695955 -1.4723168  -1.43767406 -1.40303131 -1.36838856 -1.33374581\n",
            " -1.29910306 -1.26446031 -1.22981757 -1.19517482 -1.16053207 -1.12588932\n",
            " -1.09124657 -1.05660382 -1.02196108 -0.98731833 -0.95267558 -0.91803283\n",
            " -0.88339008 -0.84874733 -0.81410459 -0.77946184 -0.74481909 -0.71017634\n",
            " -0.67553359 -0.64089084 -0.6062481  -0.57160535 -0.5369626  -0.50231985\n",
            " -0.4676771  -0.43303435 -0.39839161 -0.36374886 -0.32910611 -0.29446336\n",
            " -0.25982061 -0.22517786 -0.19053512 -0.15589237 -0.12124962 -0.08660687\n",
            " -0.05196412 -0.01732137  0.01732137  0.05196412  0.08660687  0.12124962\n",
            "  0.15589237  0.19053512  0.22517786  0.25982061  0.29446336  0.32910611\n",
            "  0.36374886  0.39839161  0.43303435  0.4676771   0.50231985  0.5369626\n",
            "  0.57160535  0.6062481   0.64089084  0.67553359  0.71017634  0.74481909\n",
            "  0.77946184  0.81410459  0.84874733  0.88339008  0.91803283  0.95267558\n",
            "  0.98731833  1.02196108  1.05660382  1.09124657  1.12588932  1.16053207\n",
            "  1.19517482  1.22981757  1.26446031  1.29910306  1.33374581  1.36838856\n",
            "  1.40303131  1.43767406  1.4723168   1.50695955  1.5416023   1.57624505\n",
            "  1.6108878   1.64553055  1.68017329  1.71481604]\n",
            "[-1.61129629 -1.66308406 -1.52246433 -1.36974056 -1.57348544 -1.53928892\n",
            " -1.25954996 -1.33527807 -1.46857292 -1.29733962 -1.39936419 -1.36548296\n",
            " -1.23546049 -1.49310635 -1.433406   -1.24178059 -1.26859415 -1.05470091\n",
            " -1.18601368 -1.2201036  -0.7962074  -0.991048   -0.9171375  -1.0850115\n",
            " -0.93160741 -0.80867859 -0.94535964 -0.7044369  -0.8024478  -0.72641944\n",
            " -0.73420381 -0.36771683 -0.58616611 -0.69336819 -0.40456952 -0.6470693\n",
            " -0.41927916 -0.67872477 -0.55902157 -0.31832149 -0.21078872 -0.25338487\n",
            " -0.25805532 -0.24897343 -0.37421267 -0.23728625 -0.16799309  0.07172036\n",
            "  0.0092995  -0.24176761  0.07504297  0.01320948  0.00788593  0.21656859\n",
            "  0.30754321  0.32823456  0.12268678  0.22864878  0.34956954  0.47100566\n",
            "  0.30821724  0.38215627  0.29168236  0.3137072   0.61990281  0.72772114\n",
            "  0.5685169   0.74834987  0.69562528  0.59349536  0.76398133  0.95750391\n",
            "  0.77858237  1.02949536  0.49708397  0.9973098   0.93199775  0.91391664\n",
            "  1.00102463  0.75365808  1.02724231  1.13953867  1.32549735  1.08939218\n",
            "  1.0842875   1.16001682  1.38610784  1.34086406  1.25880783  1.43423791\n",
            "  1.41207617  1.56428892  1.37225495  1.45714532  1.48261302  1.37172874\n",
            "  1.64419424  1.67364041  1.67317777  1.67491435]\n",
            "Training for relu:\n",
            "Weight=1.7269268034179253, Bias=-0.9483724474098119\n",
            "Normalized Test Study Hours: 1.610887798481921\n",
            "Predicted score (relu): 1.833512869087511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Temperature Prediction**"
      ],
      "metadata": {
        "id": "2HaZVeo9rQrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Generating dataset\n",
        "altitudes = np.linspace(0, 3000, 100)\n",
        "print(altitudes)\n",
        "temperature = 30 - 0.0065 * altitudes + np.random.normal(0, 2, 100)\n",
        "print(temperature)\n",
        "\n",
        "# Normalizing dataset\n",
        "norm_altitudes = (altitudes - np.mean(altitudes)) / np.std(altitudes)\n",
        "norm_temp = (temperature - np.mean(temperature)) / np.std(temperature)\n",
        "print(norm_temp)\n",
        "\n",
        "# Gradient Descent\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "learning_rate = 0.001\n",
        "\n",
        "def gradient(act_function, iters=1000):\n",
        "    global w, b\n",
        "    for i in range(iters):\n",
        "        norm_temp_pred = act_function((w * norm_altitudes) + b)\n",
        "        error = norm_temp - norm_temp_pred\n",
        "\n",
        "        dw = -(2 / len(norm_altitudes)) * np.sum(norm_altitudes * error)\n",
        "        db = -(2 / len(norm_altitudes)) * np.sum(error)\n",
        "\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "\n",
        "    return w, b\n",
        "\n",
        "# Sigmoid\n",
        "w_sigmoid, b_sigmoid = gradient(sigmoid)\n",
        "print(\"Training for sigmoid:\")\n",
        "print(f\"Weight={w_sigmoid}, Bias={b_sigmoid}\")\n",
        "\n",
        "new_altitude = 2878.78787879\n",
        "normalized = (new_altitude - np.mean(altitudes)) / np.std(altitudes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output for sigmoid=\", sigmoid((w_sigmoid * normalized) + b_sigmoid))\n",
        "print(\"\\n\")\n",
        "\n",
        "# ReLU\n",
        "w_relu, b_relu = gradient(relu)\n",
        "print(\"Training for relu:\")\n",
        "print(f\"Weight={w_relu}, Bias={b_relu}\")\n",
        "\n",
        "new_altitude =  30.3030303\n",
        "normalized = (new_altitude - np.mean(altitudes)) / np.std(altitudes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output for ReLU=\", relu((w_relu * normalized) + b_relu))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Tanh\n",
        "w_tanh, b_tanh = gradient(tanh)\n",
        "print(\"Training for tanh:\")\n",
        "print(f\"Weight={w_tanh}, Bias={b_tanh}\")\n",
        "\n",
        "new_altitude = 2878.78787879\n",
        "normalized = (new_altitude - np.mean(altitudes)) / np.std(altitudes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output=\", tanh((w_tanh * normalized) + b_tanh))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ErdLLpTvJBLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac2d67b-4f4d-4c07-f9ae-699093f29194"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0.           30.3030303    60.60606061   90.90909091  121.21212121\n",
            "  151.51515152  181.81818182  212.12121212  242.42424242  272.72727273\n",
            "  303.03030303  333.33333333  363.63636364  393.93939394  424.24242424\n",
            "  454.54545455  484.84848485  515.15151515  545.45454545  575.75757576\n",
            "  606.06060606  636.36363636  666.66666667  696.96969697  727.27272727\n",
            "  757.57575758  787.87878788  818.18181818  848.48484848  878.78787879\n",
            "  909.09090909  939.39393939  969.6969697  1000.         1030.3030303\n",
            " 1060.60606061 1090.90909091 1121.21212121 1151.51515152 1181.81818182\n",
            " 1212.12121212 1242.42424242 1272.72727273 1303.03030303 1333.33333333\n",
            " 1363.63636364 1393.93939394 1424.24242424 1454.54545455 1484.84848485\n",
            " 1515.15151515 1545.45454545 1575.75757576 1606.06060606 1636.36363636\n",
            " 1666.66666667 1696.96969697 1727.27272727 1757.57575758 1787.87878788\n",
            " 1818.18181818 1848.48484848 1878.78787879 1909.09090909 1939.39393939\n",
            " 1969.6969697  2000.         2030.3030303  2060.60606061 2090.90909091\n",
            " 2121.21212121 2151.51515152 2181.81818182 2212.12121212 2242.42424242\n",
            " 2272.72727273 2303.03030303 2333.33333333 2363.63636364 2393.93939394\n",
            " 2424.24242424 2454.54545455 2484.84848485 2515.15151515 2545.45454545\n",
            " 2575.75757576 2606.06060606 2636.36363636 2666.66666667 2696.96969697\n",
            " 2727.27272727 2757.57575758 2787.87878788 2818.18181818 2848.48484848\n",
            " 2878.78787879 2909.09090909 2939.39393939 2969.6969697  3000.        ]\n",
            "[29.47151689 29.13248604 33.01632984 29.33327528 28.32188326 29.35153169\n",
            " 29.40340218 30.46079648 30.40831753 29.30986593 27.80165005 27.30008462\n",
            " 28.98056218 28.87346351 22.99635771 22.93069603 28.02363912 27.78652868\n",
            " 26.75520361 23.74668707 27.34377702 29.13042997 23.94553226 27.07444884\n",
            " 24.9141188  23.98873554 26.28530278 24.8005124  23.51658481 21.46528203\n",
            " 26.58097063 26.53693237 22.68175674 23.65338195 23.67796126 23.70370304\n",
            " 19.33223538 23.75730132 19.83013695 23.17653313 24.51579689 21.6737282\n",
            " 18.85763829 22.0201537  22.63510961 19.79374033 18.72623075 21.493806\n",
            " 22.49076944 18.81913411 21.05581397 19.1658898  16.58244345 18.26235749\n",
            " 21.37679577 23.6774757  18.32309418 22.68067504 20.54294423 18.5321813\n",
            " 18.32617098 18.16513923 15.44197364 16.51443595 15.12641527 14.67102805\n",
            " 18.61872609 15.85813353 14.01805721 18.06939959 17.04408561 14.35583098\n",
            " 17.62798645 17.64579068 17.95286444 12.92734973 16.10888384 13.68733011\n",
            " 15.13281437 13.6769293  17.5395895  17.65050984 12.1537575  14.52228784\n",
            " 13.05783259 13.05332555 11.62840636 14.78418404 16.56500887 14.74610292\n",
            " 12.93287392 10.20279825 14.30191267 11.52745547 14.66241312 11.38059472\n",
            " 11.17182668  9.60303016 14.10032128  8.17168634]\n",
            "[ 1.54096481  1.48307095  2.14628702  1.51735829  1.34465015  1.5204758\n",
            "  1.52933335  1.70989697  1.70093552  1.51336084  1.25581367  1.17016495\n",
            "  1.45712801  1.43883954  0.43524846  0.42403589  1.29372114  1.2532315\n",
            "  1.07711954  0.56337681  1.17762598  1.48271985  0.59733217  1.13163475\n",
            "  0.76273073  0.60470969  0.99687795  0.74333098  0.5240839   0.17379768\n",
            "  1.04736702  1.03984693  0.38152632  0.54744377  0.551641    0.55603674\n",
            " -0.19044735  0.56518933 -0.10542428  0.46601572  0.69471216  0.20939254\n",
            " -0.27149088  0.26854913  0.37356072 -0.11163947 -0.2939304   0.17866852\n",
            "  0.34891279 -0.27806596  0.10387577 -0.21885299 -0.66000953 -0.3731427\n",
            "  0.15868752  0.55155808 -0.36277113  0.3813416   0.01629669 -0.32706683\n",
            " -0.36224573 -0.38974396 -0.85475936 -0.67162269 -0.90864499 -0.98640819\n",
            " -0.3122882  -0.78369473 -1.09791132 -0.40609273 -0.58117822 -1.04023213\n",
            " -0.48146968 -0.47842938 -0.4259926  -1.28416359 -0.7408759  -1.15438721\n",
            " -0.90755226 -1.15616328 -0.49656459 -0.47762352 -1.41626437 -1.01180749\n",
            " -1.26188197 -1.26265161 -1.50597481 -0.96708536 -0.66298671 -0.9735882\n",
            " -1.28322027 -1.74941565 -1.04943937 -1.52321347 -0.9878793  -1.54829182\n",
            " -1.58394164 -1.85183373 -1.08386368 -2.09625402]\n",
            "Training for sigmoid:\n",
            "Weight=-0.7026338235626621, Bias=0.7423423053695141\n",
            "Normalized value= 1.5762450491129634\n",
            "Output for sigmoid= 0.4097060261687236\n",
            "\n",
            "\n",
            "Training for relu:\n",
            "Weight=-1.4244826440727258, Bias=-0.35378791499854584\n",
            "Normalized value= -1.6801732941103025\n",
            "Output for ReLU= 2.0395897814960793\n",
            "\n",
            "\n",
            "Training for tanh:\n",
            "Weight=-1.7765833250532193, Bias=-0.17311625687872859\n",
            "Normalized value= 1.5762450491129634\n",
            "Output= -0.9947857353694035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Generating dataset\n",
        "altitudes = np.linspace(0, 3000, 100)\n",
        "print(altitudes)\n",
        "temperature = 30 - 0.0065 * altitudes + np.random.normal(0, 2, 100)\n",
        "print(temperature)\n",
        "\n",
        "# Normalizing dataset\n",
        "norm_altitudes = (altitudes - np.mean(altitudes)) / np.std(altitudes)\n",
        "norm_temp = (temperature - np.mean(temperature)) / np.std(temperature)\n",
        "print(norm_temp)\n",
        "\n",
        "# Gradient Descent\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "learning_rate = 0.001\n",
        "\n",
        "def gradient(act_function, iters=1000):\n",
        "    global w, b\n",
        "    for i in range(iters):\n",
        "        norm_temp_pred = act_function((w * norm_altitudes) + b)\n",
        "        error = norm_temp - norm_temp_pred\n",
        "\n",
        "        dw = -(2 / len(norm_altitudes)) * np.sum(norm_altitudes * error)\n",
        "        db = -(2 / len(norm_altitudes)) * np.sum(error)\n",
        "\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "\n",
        "    return w, b\n",
        "\n",
        "# Sigmoid\n",
        "w_sigmoid, b_sigmoid = gradient(sigmoid)\n",
        "print(\"Training for sigmoid:\")\n",
        "print(f\"Weight={w_sigmoid}, Bias={b_sigmoid}\")\n",
        "\n",
        "new_altitude = 40\n",
        "normalized = (new_altitude - np.mean(altitudes)) / np.std(altitudes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output for sigmoid=\", sigmoid((w_sigmoid * normalized) + b_sigmoid))\n",
        "print(\"\\n\")\n",
        "\n",
        "# ReLU\n",
        "w_relu, b_relu = gradient(relu)\n",
        "print(\"Training for relu:\")\n",
        "print(f\"Weight={w_relu}, Bias={b_relu}\")\n",
        "\n",
        "new_altitude =  40\n",
        "normalized = (new_altitude - np.mean(altitudes)) / np.std(altitudes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output for ReLU=\", relu((w_relu * normalized) + b_relu))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Tanh\n",
        "w_tanh, b_tanh = gradient(tanh)\n",
        "print(\"Training for tanh:\")\n",
        "print(f\"Weight={w_tanh}, Bias={b_tanh}\")\n",
        "\n",
        "new_altitude = 40\n",
        "normalized = (new_altitude - np.mean(altitudes)) / np.std(altitudes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output=\", tanh((w_tanh * normalized) + b_tanh))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40czbvJttslY",
        "outputId": "0dc6014b-ba90-47b2-94e7-902da44561a6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0.           30.3030303    60.60606061   90.90909091  121.21212121\n",
            "  151.51515152  181.81818182  212.12121212  242.42424242  272.72727273\n",
            "  303.03030303  333.33333333  363.63636364  393.93939394  424.24242424\n",
            "  454.54545455  484.84848485  515.15151515  545.45454545  575.75757576\n",
            "  606.06060606  636.36363636  666.66666667  696.96969697  727.27272727\n",
            "  757.57575758  787.87878788  818.18181818  848.48484848  878.78787879\n",
            "  909.09090909  939.39393939  969.6969697  1000.         1030.3030303\n",
            " 1060.60606061 1090.90909091 1121.21212121 1151.51515152 1181.81818182\n",
            " 1212.12121212 1242.42424242 1272.72727273 1303.03030303 1333.33333333\n",
            " 1363.63636364 1393.93939394 1424.24242424 1454.54545455 1484.84848485\n",
            " 1515.15151515 1545.45454545 1575.75757576 1606.06060606 1636.36363636\n",
            " 1666.66666667 1696.96969697 1727.27272727 1757.57575758 1787.87878788\n",
            " 1818.18181818 1848.48484848 1878.78787879 1909.09090909 1939.39393939\n",
            " 1969.6969697  2000.         2030.3030303  2060.60606061 2090.90909091\n",
            " 2121.21212121 2151.51515152 2181.81818182 2212.12121212 2242.42424242\n",
            " 2272.72727273 2303.03030303 2333.33333333 2363.63636364 2393.93939394\n",
            " 2424.24242424 2454.54545455 2484.84848485 2515.15151515 2545.45454545\n",
            " 2575.75757576 2606.06060606 2636.36363636 2666.66666667 2696.96969697\n",
            " 2727.27272727 2757.57575758 2787.87878788 2818.18181818 2848.48484848\n",
            " 2878.78787879 2909.09090909 2939.39393939 2969.6969697  3000.        ]\n",
            "[26.84228549 28.77962106 31.18901428 31.49844409 32.56846104 31.52630187\n",
            " 27.40340683 29.39032797 30.98817883 26.18306488 27.12043002 29.35820907\n",
            " 31.66244075 30.26487457 28.05707382 27.41275081 26.77617057 22.96640503\n",
            " 28.20894553 26.55235634 27.02748451 27.80627082 25.47901461 26.60179156\n",
            " 23.50528401 24.77724998 24.88279895 26.71466015 24.97034088 22.23856589\n",
            " 25.60669392 25.20534727 26.78224359 27.7499873  20.84777957 18.87917952\n",
            " 21.17836081 23.95505261 22.8453195  24.3696365  20.15253385 22.60315425\n",
            " 21.84835233 22.10224023 19.40555396 22.14844439 21.52286445 21.13768695\n",
            " 21.60257498 21.97965787 20.42447804 20.63421136 20.04772071 18.33550722\n",
            " 17.40758877 17.94121497 16.27641396 19.01458541 18.13098657 18.43093777\n",
            " 18.95590255 19.26957501 15.04045233 18.70621511 18.71761136 18.22832674\n",
            " 18.80308522 18.48662252 14.93474171 15.93672092 21.20888089 19.77387784\n",
            " 10.10339418 18.7209933  15.15605508 14.93124254 11.52462485 11.91790035\n",
            " 13.83526053 10.59917404 18.65716259 18.27401957 14.9177223  13.43778793\n",
            "  9.61577604 14.10149356 13.05881509 13.4777377  14.94618306 13.79266875\n",
            " 13.46034718 13.6832666   9.87500929  9.60790024  8.8194505  11.65924591\n",
            " 11.72584295 12.03733423 10.21333341 11.22166617]\n",
            "[ 1.02229953  1.33396124  1.72156349  1.77134187  1.9434769   1.77582338\n",
            "  1.11256785  1.43220646  1.68925483  0.91624985  1.067045    1.42703945\n",
            "  1.79772422  1.57289592  1.21772412  1.11407103  1.01166353  0.39878156\n",
            "  1.24215592  0.97565825  1.05209274  1.17737711  0.80298836  0.98361096\n",
            "  0.48547173  0.69009456  0.70707436  1.00176827  0.72115735  0.28169312\n",
            "  0.82352829  0.75896313  1.01264051  1.16832271  0.0579555  -0.25873577\n",
            "  0.11113654  0.55782659  0.37930237  0.62452125 -0.05388959  0.34034493\n",
            "  0.21891896  0.25976223 -0.17405722  0.26719516  0.16655729  0.10459329\n",
            "  0.17938043  0.24004225 -0.01014157  0.02359851 -0.07075101 -0.34619704\n",
            " -0.4954725  -0.40962735 -0.67744607 -0.23695284 -0.37909855 -0.330845\n",
            " -0.24639323 -0.19593233 -0.87627684 -0.28656078 -0.28472745 -0.36343931\n",
            " -0.27097716 -0.32188693 -0.89328265 -0.73209294  0.11604635 -0.11480447\n",
            " -1.67050786 -0.28418339 -0.85767967 -0.89384556 -1.44187261 -1.37860587\n",
            " -1.07015762 -1.5907511  -0.29445192 -0.35608864 -0.89602058 -1.13409956\n",
            " -1.74895162 -1.02732837 -1.19506542 -1.12767279 -0.89144206 -1.07700942\n",
            " -1.13047043 -1.09460909 -1.70724843 -1.75021862 -1.87705756 -1.42021595\n",
            " -1.4095024  -1.35939238 -1.65282179 -1.49060997]\n",
            "Training for sigmoid:\n",
            "Weight=-1.8402091713975743, Bias=-1.5004933390295225\n",
            "Normalized value= -1.6690876146405662\n",
            "Output for sigmoid= 0.8279228423553473\n",
            "\n",
            "\n",
            "Training for relu:\n",
            "Weight=-2.819628295314763, Bias=-2.146139484273063\n",
            "Normalized value= -1.6690876146405662\n",
            "Output for ReLU= 2.5600671813269016\n",
            "\n",
            "\n",
            "Training for tanh:\n",
            "Weight=-3.230730094196548, Bias=-1.4677821878063635\n",
            "Normalized value= -1.6690876146405662\n",
            "Output= 0.9992201598637983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**3.house price prediction**"
      ],
      "metadata": {
        "id": "n6R_0sOTzVZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Generating dataset\n",
        "sizes = np.linspace(100, 3000, 100)\n",
        "print(sizes)\n",
        "prices =  100 * sizes + np.random.normal(0, 2, 100)\n",
        "print(prices)\n",
        "\n",
        "# Normalizing dataset\n",
        "norm_sizes = (sizes - np.mean(sizes)) / np.std(sizes)\n",
        "norm_prices = (prices - np.mean(prices)) / np.std(prices)\n",
        "print(norm_prices)\n",
        "\n",
        "# Gradient Descent\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "learning_rate = 0.001\n",
        "\n",
        "def gradient(act_function, iters=1000):\n",
        "    global w, b\n",
        "    for i in range(iters):\n",
        "        norm_prices_pred = act_function((w * norm_sizes) + b)\n",
        "        error = norm_prices - norm_prices_pred\n",
        "\n",
        "        dw = -(2 / len(norm_sizes)) * np.sum(norm_sizes * error)\n",
        "        db = -(2 / len(norm_sizes)) * np.sum(error)\n",
        "\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "\n",
        "    return w, b\n",
        "\n",
        "# Sigmoid\n",
        "w_sigmoid, b_sigmoid = gradient(sigmoid)\n",
        "print(\"Training for sigmoid:\")\n",
        "print(f\"Weight={w_sigmoid}, Bias={b_sigmoid}\")\n",
        "\n",
        "new_size =  228.28282828\n",
        "normalized = (new_size - np.mean(sizes)) / np.std(sizes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output for sigmoid=\", sigmoid((w_sigmoid * normalized) + b_sigmoid))\n",
        "print(\"\\n\")\n",
        "\n",
        "# ReLU\n",
        "w_relu, b_relu = gradient(relu)\n",
        "print(\"Training for relu:\")\n",
        "print(f\"Weight={w_relu}, Bias={b_relu}\")\n",
        "\n",
        "new_size =   228.28282828\n",
        "normalized = (new_size - np.mean(sizes)) / np.std(sizes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output for ReLU=\", relu((w_relu * normalized) + b_relu))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Tanh\n",
        "w_tanh, b_tanh = gradient(tanh)\n",
        "print(\"Training for tanh:\")\n",
        "print(f\"Weight={w_tanh}, Bias={b_tanh}\")\n",
        "\n",
        "new_size =  228.28282828\n",
        "normalized = (new_size - np.mean(sizes)) / np.std(sizes)\n",
        "print(\"Normalized value=\", normalized)\n",
        "print(\"Output=\", tanh((w_tanh * normalized) + b_tanh))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JejnyyM0yT2C",
        "outputId": "1f01a712-7fe1-4ae9-86b3-5315cddaaefb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 100.          129.29292929  158.58585859  187.87878788  217.17171717\n",
            "  246.46464646  275.75757576  305.05050505  334.34343434  363.63636364\n",
            "  392.92929293  422.22222222  451.51515152  480.80808081  510.1010101\n",
            "  539.39393939  568.68686869  597.97979798  627.27272727  656.56565657\n",
            "  685.85858586  715.15151515  744.44444444  773.73737374  803.03030303\n",
            "  832.32323232  861.61616162  890.90909091  920.2020202   949.49494949\n",
            "  978.78787879 1008.08080808 1037.37373737 1066.66666667 1095.95959596\n",
            " 1125.25252525 1154.54545455 1183.83838384 1213.13131313 1242.42424242\n",
            " 1271.71717172 1301.01010101 1330.3030303  1359.5959596  1388.88888889\n",
            " 1418.18181818 1447.47474747 1476.76767677 1506.06060606 1535.35353535\n",
            " 1564.64646465 1593.93939394 1623.23232323 1652.52525253 1681.81818182\n",
            " 1711.11111111 1740.4040404  1769.6969697  1798.98989899 1828.28282828\n",
            " 1857.57575758 1886.86868687 1916.16161616 1945.45454545 1974.74747475\n",
            " 2004.04040404 2033.33333333 2062.62626263 2091.91919192 2121.21212121\n",
            " 2150.50505051 2179.7979798  2209.09090909 2238.38383838 2267.67676768\n",
            " 2296.96969697 2326.26262626 2355.55555556 2384.84848485 2414.14141414\n",
            " 2443.43434343 2472.72727273 2502.02020202 2531.31313131 2560.60606061\n",
            " 2589.8989899  2619.19191919 2648.48484848 2677.77777778 2707.07070707\n",
            " 2736.36363636 2765.65656566 2794.94949495 2824.24242424 2853.53535354\n",
            " 2882.82828283 2912.12121212 2941.41414141 2970.70707071 3000.        ]\n",
            "[ 10002.93054853  12929.23695573  15858.82028329  18788.38184523\n",
            "  21714.13100836  24646.49708606  27574.95992723  30505.33789431\n",
            "  33436.18577789  36362.86107526  39290.47914112  42216.97203221\n",
            "  45155.61461027  48080.35493222  51012.10424958  53939.27208847\n",
            "  56868.2995402   59798.3038889   62726.09853041  65656.05690086\n",
            "  68582.41507815  71516.22919273  74444.82494404  77372.19229449\n",
            "  80300.7254505   83231.52928605  86161.17830056  89089.35159635\n",
            "  92020.28935217  94949.00734312  97878.04650805 100808.58687306\n",
            " 103733.83553017 106667.62555216 109598.70280701 112525.66505486\n",
            " 115454.14783137 118383.42104331 121311.47465182 124240.52486619\n",
            " 127170.66067351 130104.72451127 133030.39927142 135958.58920793\n",
            " 138891.10810953 141818.13613981 144743.39289321 147675.30683037\n",
            " 150606.82170927 153533.39564056 156465.17270503 159391.25627383\n",
            " 162317.79188915 165252.23424469 168181.27800796 171112.25286112\n",
            " 174043.07270705 176973.24207175 179898.80027809 182830.82764312\n",
            " 185756.91396679 188686.99488245 191616.51516387 194545.7349403\n",
            " 197476.83481808 200402.80121799 203336.22745777 206263.95816994\n",
            " 209192.53838202 212119.89013697 215051.64708604 217981.6546418\n",
            " 220909.42984038 223842.24974477 226767.44235003 229697.62632892\n",
            " 232627.58321611 235556.16079124 238483.64206016 241411.98249156\n",
            " 244345.48396945 247269.50628393 250201.64398659 253130.1640772\n",
            " 256063.48790461 258990.67582332 261921.63536064 264848.88266134\n",
            " 267777.02778484 270705.17825003 273637.60023072 276569.31315066\n",
            " 279491.74045155 282425.39153662 285355.61987281 288282.83882878\n",
            " 291212.91589898 294142.01369892 297067.55442853 300001.78305428]\n",
            "[-1.71477528 -1.68016798 -1.64552192 -1.61087611 -1.5762754  -1.54159642\n",
            " -1.50696362 -1.47230816 -1.43764714 -1.40303547 -1.36841265 -1.33380314\n",
            " -1.29904994 -1.26446116 -1.22978948 -1.19517199 -1.1605325  -1.12588146\n",
            " -1.09125655 -1.05660605 -1.02199814 -0.98730204 -0.95266766 -0.9180478\n",
            " -0.88341416 -0.84875367 -0.81410683 -0.77947745 -0.74481537 -0.71017954\n",
            " -0.67553991 -0.64088254 -0.60628774 -0.57159193 -0.5369282  -0.50231314\n",
            " -0.46768009 -0.4330377  -0.39840973 -0.36376997 -0.32911738 -0.29441833\n",
            " -0.25981849 -0.22518891 -0.19050813 -0.15589229 -0.1212974  -0.08662377\n",
            " -0.05195487 -0.0173444   0.01732761  0.05193228  0.08654229  0.12124582\n",
            "  0.1558855   0.19054802  0.2252087   0.25986169  0.29446015  0.32913512\n",
            "  0.36373982  0.39839177  0.43303708  0.46767884  0.50234284  0.53694612\n",
            "  0.57163763  0.60626178  0.64089598  0.67551565  0.71018742  0.74483849\n",
            "  0.77946317  0.81414751  0.84874164  0.88339481  0.91804529  0.95267945\n",
            "  0.98730065  1.02193202  1.05662441  1.09120471  1.12588098  1.16051447\n",
            "  1.19520476  1.22982249  1.26448483  1.29910327  1.33373232  1.36836143\n",
            "  1.40304106  1.43771231  1.47227374  1.50696791  1.5416216   1.5762397\n",
            "  1.6108916   1.64553192  1.68013017  1.71483116]\n",
            "Training for sigmoid:\n",
            "Weight=1.6568110696037708, Bias=-0.16749639819751247\n",
            "Normalized value= -1.563104696298259\n",
            "Output for sigmoid= 0.059677627642833686\n",
            "\n",
            "\n",
            "Training for relu:\n",
            "Weight=2.3070345497583205, Bias=-1.2302076703069933\n",
            "Normalized value= -1.563104696298259\n",
            "Output for ReLU= 0.0\n",
            "\n",
            "\n",
            "Training for tanh:\n",
            "Weight=2.7398219420407735, Bias=-0.7793594014602947\n",
            "Normalized value= -1.563104696298259\n",
            "Output= -0.9999197905177194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rZhhdIl02JT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}