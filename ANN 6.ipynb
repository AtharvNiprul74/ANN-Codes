{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIK4+MwENgPJxI0fkFxl9D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtharvNiprul74/ANN-Codes/blob/main/ANN%206.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw83ZsRhZe6g",
        "outputId": "8a27b77c-c8cf-4707-ac03-a8760dd37143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n",
            "Hidden Output:\n",
            "[[0.5464309  0.58554065]\n",
            " [0.54645924 0.65653557]\n",
            " [0.64640687 0.74381361]\n",
            " [0.64643301 0.79709206]]\n",
            "Final Output:\n",
            "[[0.62972271]\n",
            " [0.63125095]\n",
            " [0.63652524]\n",
            " [0.63766357]]\n",
            "\n",
            "Epoch 1000:\n",
            "Hidden Output:\n",
            "[[0.5579081  0.39919166]\n",
            " [0.58575494 0.93222693]\n",
            " [0.7789187  0.94044095]\n",
            " [0.79788756 0.9969502 ]]\n",
            "Final Output:\n",
            "[[0.27495903]\n",
            " [0.62740994]\n",
            " [0.56665297]\n",
            " [0.59948497]]\n",
            "\n",
            "Epoch 2000:\n",
            "Hidden Output:\n",
            "[[0.00505681 0.09088796]\n",
            " [0.13771819 0.96467933]\n",
            " [0.1399586  0.96796418]\n",
            " [0.83643572 0.99987887]]\n",
            "Final Output:\n",
            "[[0.07314159]\n",
            " [0.93073686]\n",
            " [0.93112166]\n",
            " [0.07577059]]\n",
            "\n",
            "Epoch 3000:\n",
            "Hidden Output:\n",
            "[[0.00255616 0.07283786]\n",
            " [0.1106966  0.96738949]\n",
            " [0.11220544 0.96961392]\n",
            " [0.85992131 0.99991702]]\n",
            "Final Output:\n",
            "[[0.04618646]\n",
            " [0.9583158 ]\n",
            " [0.95850025]\n",
            " [0.04433005]]\n",
            "\n",
            "Epoch 4000:\n",
            "Hidden Output:\n",
            "[[0.00186498 0.06586844]\n",
            " [0.10013911 0.9689941 ]\n",
            " [0.10136526 0.97080074]\n",
            " [0.87043491 0.99993214]]\n",
            "Final Output:\n",
            "[[0.0361755 ]\n",
            " [0.9678866 ]\n",
            " [0.96801067]\n",
            " [0.03380245]]\n",
            "\n",
            "Epoch 5000:\n",
            "Hidden Output:\n",
            "[[0.00152524 0.06180989]\n",
            " [0.09395766 0.97009933]\n",
            " [0.09502308 0.97166796]\n",
            " [0.87697031 0.99994079]]\n",
            "Final Output:\n",
            "[[0.03063173]\n",
            " [0.97305059]\n",
            " [0.97314524]\n",
            " [0.02820621]]\n",
            "\n",
            "Epoch 6000:\n",
            "Hidden Output:\n",
            "[[0.00131783 0.05903232]\n",
            " [0.08971419 0.97093199]\n",
            " [0.09067245 0.97234199]\n",
            " [0.88162111 0.99994658]]\n",
            "Final Output:\n",
            "[[0.02700514]\n",
            " [0.97637916]\n",
            " [0.97645628]\n",
            " [0.02463056]]\n",
            "\n",
            "Epoch 7000:\n",
            "Hidden Output:\n",
            "[[0.00117569 0.05695794]\n",
            " [0.08653878 0.97159502]\n",
            " [0.08741899 0.97288958]\n",
            " [0.88518835 0.9999508 ]]\n",
            "Final Output:\n",
            "[[0.02440209]\n",
            " [0.97874489]\n",
            " [0.97881032]\n",
            " [0.02210435]]\n",
            "\n",
            "Epoch 8000:\n",
            "Hidden Output:\n",
            "[[0.00107102 0.05532142]\n",
            " [0.08403023 0.97214309]\n",
            " [0.08485025 0.97334874]\n",
            " [0.8880585  0.99995405]]\n",
            "Final Output:\n",
            "[[0.02241944]\n",
            " [0.98053393]\n",
            " [0.98059096]\n",
            " [0.02020236]]\n",
            "\n",
            "Epoch 9000:\n",
            "Hidden Output:\n",
            "[[9.90057454e-04 5.39808152e-02]\n",
            " [8.19732702e-02 9.72608473e-01]\n",
            " [8.27450340e-02 9.73742880e-01]\n",
            " [8.90445745e-01 9.99956669e-01]]\n",
            "Final Output:\n",
            "[[0.02084565]\n",
            " [0.98194622]\n",
            " [0.98199691]\n",
            " [0.01870604]]\n",
            "\n",
            "Predicted output values:\n",
            "[[0.01955784]\n",
            " [0.9830968 ]\n",
            " [0.98314253]\n",
            " [0.01749036]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "X=np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y=np.array([[0], [1], [1], [0]])\n",
        "\n",
        "np.random.seed(1)\n",
        "input_neurons = 2\n",
        "hidden_neurons = 2\n",
        "output_neurons = 1\n",
        "\n",
        "np.random.seed(1)\n",
        "weights_input_hidden = np.random.rand(input_neurons, hidden_neurons)\n",
        "weights_hidden_output = np.random.rand(hidden_neurons, output_neurons)\n",
        "\n",
        "bias_hidden = np.random.rand(1, hidden_neurons)\n",
        "bias_output = np.random.rand(1, output_neurons)\n",
        "\n",
        "learning_rate = 0.5\n",
        "epochs=10000\n",
        "\n",
        "for i in range(epochs):\n",
        "    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
        "    hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
        "    final_output = sigmoid(final_input)\n",
        "\n",
        "    error = Y - final_output\n",
        "\n",
        "    gd_output = error * sigmoid_derivative(final_output)\n",
        "    gd_hidden = np.dot(gd_output, weights_hidden_output.T) * sigmoid_derivative(hidden_output)\n",
        "\n",
        "    weights_hidden_output += np.dot(hidden_output.T, gd_output) * learning_rate\n",
        "    bias_output += np.sum(gd_output, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    weights_input_hidden += np.dot(X.T, gd_hidden) * learning_rate\n",
        "    bias_hidden += np.sum(gd_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        print(f\"Epoch {i}:\\nHidden Output:\\n{hidden_output}\\nFinal Output:\\n{final_output}\\n\")\n",
        "\n",
        "predictions = sigmoid(np.dot(sigmoid(np.dot(X, weights_input_hidden) + bias_hidden), weights_hidden_output) + bias_output)\n",
        "print(\"Predicted output values:\")\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df=pd.read_excel('Student_dataset.xlsx')\n",
        "\n",
        "# initializing parameters\n",
        "def init_parameters(layers_dimension):\n",
        "  print(layers_dimension)\n",
        "  L=len(layers_dimension)\n",
        "  parameters={}\n",
        "  for i in range (1,L):\n",
        "    parameters['W'+str(i)]=np.ones((layers_dimension[i-1],layers_dimension[i]))*0.1\n",
        "    parameters['b'+str(i)]=np.zeros((layers_dimension[i],1))\n",
        "\n",
        "  return parameters\n",
        "\n",
        "# summation function\n",
        "def linear_forward(a_prev,W,b):\n",
        "  Z=np.dot(W.T,a_prev)+b\n",
        "  return Z\n",
        "\n",
        "# activation function\n",
        "def ReLU(Z):\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "# Derivative of ReLU\n",
        "def dReLU(Z):\n",
        "    return Z > 0\n",
        "\n",
        "# sigmoid for output\n",
        "def sigmoid(Z):\n",
        "  return 1/(1+np.exp(-Z))\n",
        "\n",
        "\n",
        "# feedforward propagation\n",
        "def feed_forward(X,parameters):\n",
        "  A=X\n",
        "  caches=[]\n",
        "  L=len(parameters)//2\n",
        "  for i in range (1,L):\n",
        "    A_prev=A\n",
        "    W=parameters['W'+str(i)]\n",
        "    b=parameters['b'+str(i)]\n",
        "    Z=linear_forward(A_prev,W,b)\n",
        "    A=ReLU(Z)\n",
        "    cache=(A_prev,W,b,Z)\n",
        "    caches.append(cache)\n",
        "\n",
        "  # output layer\n",
        "  W_out=parameters['W'+str(L)]\n",
        "  b_out=parameters['b'+str(L)]\n",
        "  Z_out=linear_forward(A,W_out,b_out)\n",
        "  cache=(A,W_out,b_out,Z_out)\n",
        "  caches.append(cache)\n",
        "\n",
        "  sigmoid_output=sigmoid(Z_out)\n",
        "  Y_pred=sigmoid_output\n",
        "\n",
        "  return Y_pred,caches\n",
        "\n",
        "# Compute gradients and update parameters\n",
        "def backpropagation(X, Y_true, Y_pred, caches, parameters, learning_rate):\n",
        "    grads = {}\n",
        "    L = len(caches)+1   # total number of layers\n",
        "    m = X.shape[1]       # number of examples\n",
        "\n",
        "    # loss function binary cross entropy\n",
        "    eps=1e-15\n",
        "    loss=-np.mean(Y_true * np.log(Y_pred+eps)+(1-Y_true)*np.log(1-Y_pred+eps))\n",
        "\n",
        "    # Derivative of loss w.r.t final layer output i.e sigmoid here\n",
        "    dZ = Y_pred - Y_true\n",
        "    A_prev, W, b, Z = caches[-1]\n",
        "    grads[\"dW\" + str(L)] = (1 / m) * np.dot(dZ,A_prev.T)\n",
        "    grads[\"db\" + str(L)] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W, dZ)\n",
        "\n",
        "    # Hidden layers\n",
        "    for l in reversed(range(1, L)):\n",
        "        print(L)\n",
        "        A_prev, W, b, Z = caches[l - 1]\n",
        "        dZ = dA_prev * dReLU(Z)\n",
        "        grads[\"dW\" + str(l)] = (1 / m) * np.dot(dZ,A_prev.T)\n",
        "        grads[\"db\" + str(l)] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = np.dot(W.T,dZ)\n",
        "\n",
        "    # Update parameters\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
        "\n",
        "    return parameters,loss\n",
        "\n",
        "# input features from excel file\n",
        "X=df[['CGPA','10th Score','12th Score','IQ']].values.T\n",
        "y=df[['Placement']].values.T\n",
        "# print(X)\n",
        "# init parameters\n",
        "input_features=X.shape[0]\n",
        "parameters=init_parameters([input_features,4,3,1])\n",
        "# forward propagation\n",
        "for i in range(1000):\n",
        "    Y_pred, caches = feed_forward(X, parameters)\n",
        "    parameters,loss = backpropagation(X, y, Y_pred, caches, parameters, learning_rate=0.01)\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Epoch {i}, Loss: {loss}\")\n",
        "# final prediction\n",
        "final_pred=(Y_pred > 0.5).astype(int)\n",
        "print(f\"Final Predictions:{final_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "_KVAbYE6uf8J",
        "outputId": "7614adb2-c2da-47be-9b38-a65f7cd3eda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 4, 3, 1]\n",
            "4\n",
            "4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (3,4) and (3,10) not aligned: 4 (dim 1) != 3 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-d42c12197c1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i}, Loss: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-d42c12197c1f>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(X, Y_true, Y_pred, caches, parameters, learning_rate)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (3,4) and (3,10) not aligned: 4 (dim 1) != 3 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#cols=['CGPA','10th Score','12th Score','IQ']\n",
        "#df=pd.read_excel('Student_dataset.xlsx',usecols=cols)\n",
        "\n",
        "# initializing parameters\n",
        "def init_parameters(layers_dimension):\n",
        "  print(layers_dimension)\n",
        "  L=len(layers_dimension)\n",
        "  parameters={}\n",
        "  for i in range (1,L):\n",
        "    parameters['W'+str(i)]=np.ones((layers_dimension[i],layers_dimension[i-1]))*0.1\n",
        "    parameters['b'+str(i)]=np.zeros((layers_dimension[i],1))\n",
        "\n",
        "  return parameters\n",
        "\n",
        "\n",
        "# feedforward propagation\n",
        "def feed_forward(X,parameters):\n",
        "  A=X\n",
        "  caches=[]\n",
        "  L=len(parameters)//2\n",
        "  for i in range (1,L):\n",
        "    A_prev=A\n",
        "    W=parameters['W'+str(i)]\n",
        "    b=parameters['b'+str(i)]\n",
        "    Z=linear_forward(A_prev,W,b)\n",
        "    A=ReLU(Z)\n",
        "    cache=(A_prev,W,b,Z)\n",
        "    caches.append(cache)\n",
        "\n",
        "  # output layer\n",
        "  W_out=parameters['W'+str(L)]\n",
        "  b_out=parameters['b'+str(L)]\n",
        "  Z_out=linear_forward(A,W_out,b_out)\n",
        "\n",
        "  Y_pred=Z_out\n",
        "\n",
        "  return Y_pred,caches\n",
        "\n",
        "# Compute gradients and update parameters\n",
        "def backpropagation(X, Y_true, Y_pred, caches, parameters, learning_rate):\n",
        "    grads = {}\n",
        "    L = len(caches) + 1  # total number of layers\n",
        "    m = X.shape[1]       # number of examples\n",
        "\n",
        "    # Derivative of loss w.r.t final layer output\n",
        "    dZ = (Y_pred - Y_true)  # MSE derivative\n",
        "\n",
        "    # Output layer gradients\n",
        "    A_prev, W, b, Z = caches[-1]\n",
        "    grads[\"dW\" + str(L)] = (1/m) * np.dot(dZ,A_prev.T)\n",
        "    grads[\"db\" + str(L)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    # Hidden layers\n",
        "    for l in reversed(range(1, L-1)):\n",
        "        A_prev, W, b, Z = caches[l-1]\n",
        "        dZ = dA_prev * dReLU(Z)\n",
        "        grads[\"dW\" + str(l+1)] = (1/m) * np.dot(dZ,A_prev.T)\n",
        "        grads[\"db\" + str(l+1)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = np.dot(W, dZ)\n",
        "\n",
        "    # Update parameters\n",
        "    for l in range(1, L+1):\n",
        "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# summation function\n",
        "def linear_forward(a_prev,W,b):\n",
        "  Z=np.dot(W,a_prev)+b\n",
        "  return Z\n",
        "\n",
        "# activation function\n",
        "def ReLU(Z):\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "def dReLU(Z):\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "\n",
        "# input features from excel\n",
        "data = [\n",
        "    [8.5, 85, 88, 120, 1],\n",
        "    [7.2, 78, 74, 110, 0],\n",
        "    [9.1, 90, 92, 130, 1],\n",
        "    [6.8, 70, 65, 105, 0],\n",
        "    [7.5, 75, 78, 115, 0],\n",
        "    [8.0, 80, 81, 118, 1],\n",
        "    [7.9, 79, 77, 113, 1],\n",
        "    [8.3, 83, 85, 125, 1],\n",
        "    [6.5, 65, 60, 100, 0],\n",
        "    [9.0, 92, 95, 135, 1]\n",
        "]\n",
        "data = np.array(data).T\n",
        "X=data[:-1, :]  # First 4 rows: features\n",
        "y = data[-1:, :]\n",
        "# print(X)\n",
        "# init parameters\n",
        "input_features=X.shape[0]\n",
        "parameters=init_parameters([input_features,4,3,1])\n",
        "# forward propagation\n",
        "pred_value,cache=feed_forward(X,parameters)\n",
        "#print(\"Predicted Value after Feedforward propagation=\",pred_value)\n",
        "for i in range(1000):\n",
        "    Y_pred, caches = feed_forward(X, parameters)\n",
        "    loss = np.mean((Y_pred - y) ** 2)\n",
        "    parameters = backpropagation(X, y, Y_pred, caches, parameters, learning_rate=0.01)\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Epoch {i}, Loss: {loss}\")"
      ],
      "metadata": {
        "id": "XGTFOgBlyGE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "c2cc7731-9022-477e-f73d-3851ba35d776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 4, 3, 1]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (4,3) and (1,10) not aligned: 3 (dim 1) != 1 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-7e792ced969f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i}, Loss: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-7e792ced969f>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(X, Y_true, Y_pred, caches, parameters, learning_rate)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (4,3) and (1,10) not aligned: 3 (dim 1) != 1 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel('Student_dataset.xlsx')\n",
        "\n",
        "# Initialize parameters function\n",
        "def init_parameters(layers_dimension):\n",
        "    L = len(layers_dimension)\n",
        "    parameters = {}\n",
        "    for i in range(1, L):\n",
        "        parameters['W' + str(i)] = np.ones((layers_dimension[i-1], layers_dimension[i])) * 0.1\n",
        "        parameters['b' + str(i)] = np.zeros((layers_dimension[i], 1))\n",
        "    return parameters\n",
        "\n",
        "# Summation function\n",
        "def linear_forward(a_prev, W, b):\n",
        "    Z = np.dot(W.T, a_prev) + b  # Forward linear computation\n",
        "    return Z\n",
        "\n",
        "# ReLU activation function\n",
        "def ReLU(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "# Derivative of ReLU\n",
        "def dReLU(Z):\n",
        "    return Z > 0  # Gradient of ReLU: derivative = 1 for positive, 0 for negative Z\n",
        "\n",
        "# Sigmoid for output\n",
        "def sigmoid(Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "# Feedforward propagation\n",
        "def feed_forward(X, parameters):\n",
        "    A = X\n",
        "    caches = []\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "    for i in range(1, L):\n",
        "        A_prev = A\n",
        "        W = parameters['W' + str(i)]\n",
        "        b = parameters['b' + str(i)]\n",
        "        Z = linear_forward(A_prev, W, b)\n",
        "        A = ReLU(Z)\n",
        "        caches.append((A_prev, W, b, Z))\n",
        "\n",
        "    # Output layer (sigmoid activation)\n",
        "    W_out = parameters['W' + str(L)]\n",
        "    b_out = parameters['b' + str(L)]\n",
        "    Z_out = linear_forward(A, W_out, b_out)\n",
        "    caches.append((A, W_out, b_out, Z_out))\n",
        "\n",
        "    sigmoid_output = sigmoid(Z_out)\n",
        "    Y_pred = sigmoid_output\n",
        "\n",
        "    return Y_pred, caches\n",
        "\n",
        "# Compute gradients and update parameters\n",
        "def backpropagation(X, Y_true, Y_pred, caches, parameters, learning_rate):\n",
        "    grads = {}\n",
        "    L = len(caches) + 1  # Total number of layers\n",
        "    m = X.shape[1]  # Number of examples\n",
        "\n",
        "    # Binary cross-entropy loss function\n",
        "    eps = 1e-15\n",
        "    loss = -np.mean(Y_true * np.log(Y_pred + eps) + (1 - Y_true) * np.log(1 - Y_pred + eps))\n",
        "\n",
        "    # Derivative of loss w.r.t final layer output (Sigmoid activation)\n",
        "    dZ = Y_pred - Y_true\n",
        "    A_prev, W, b, Z = caches[-1]\n",
        "    grads[\"dW\" + str(L)] = (1 / m) * np.dot(dZ, A_prev.T)\n",
        "    grads[\"db\" + str(L)] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)  # Gradient of A_prev\n",
        "\n",
        "    # Backpropagation for hidden layers\n",
        "    for l in reversed(range(1, L)):\n",
        "        A_prev, W, b, Z = caches[l - 1]\n",
        "        dZ = dA_prev * dReLU(Z)  # Apply ReLU derivative\n",
        "        grads[\"dW\" + str(l)] = (1 / m) * np.dot(dZ, A_prev.T)\n",
        "        grads[\"db\" + str(l)] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = np.dot(W.T, dZ)  # Backpropagate the gradient\n",
        "\n",
        "    # Update parameters\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
        "\n",
        "    return parameters, loss\n",
        "\n",
        "# Input features and labels from Excel file\n",
        "X = df[['CGPA', '10th Score', '12th Score', 'IQ']].values\n",
        "y = df[['Placement']].values\n",
        "\n",
        "# Initialize parameters (example: [input features, hidden layer 1, hidden layer 2, output layer])\n",
        "input_features = X.shape[0]\n",
        "parameters = init_parameters([input_features, 4, 3, 1])\n",
        "\n",
        "# Training loop\n",
        "for i in range(1000):\n",
        "    Y_pred, caches = feed_forward(X, parameters)  # Feedforward\n",
        "    parameters, loss = backpropagation(X, y, Y_pred, caches, parameters, learning_rate=0.01)  # Backpropagation\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Epoch {i}, Loss: {loss}\")\n",
        "\n",
        "# Final predictions after training\n",
        "final_pred = (Y_pred > 0.5).astype(int)\n",
        "print(f\"Final Predictions: {final_pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "SqcIkPlwlHRi",
        "outputId": "d565d30f-11f6-43dd-c35a-d90c33296238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (1,3) and (10,4) not aligned: 3 (dim 1) != 10 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-7b3c05a54910>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Feedforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-7b3c05a54910>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(X, Y_true, Y_pred, caches, parameters, learning_rate)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient of A_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Backpropagation for hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (1,3) and (10,4) not aligned: 3 (dim 1) != 10 (dim 0)"
          ]
        }
      ]
    }
  ]
}